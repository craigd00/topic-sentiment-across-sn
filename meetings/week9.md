## 23/11/21

### What I had done since last meeting
* Now using the Pushshift API for reddit posts
* VADER has a slight affect on how it classifies positive/negative posts when using capitals
* Ran test runs for topics within same time frame on both
* Should the topics I run for be random rather than just politics?
* Need to sanitise the data from reddit so that it does not contain removed, deleted or the bot posts
* Made a brief jupyter notebook to see percentage of posts that were positive/negative

### What topics should we be looking at?
* We now have the tech to acquire the sentiment analysis, provides proof of concept
* How do we want to constrain the topics? 
* Could we take what we have to identify misinformation campaigns, pro/anti vaccination
* Would be interesting research, taking sentiment analysis to see disinformation campaigns

### Disinformation in Twitter/Reddit
* Reddit API - could investigate disinformation amongst subreddits, is there pro/anti vaccine subreddits we can identify?
* Try and associate info from that, i.e. top subreddits, if positive sentiment, are they pro vax?
* Should be able to identify sub communities, turf wars in subreddit
* Takes problem and applies it to real world problems

### Validity
* Do not need to worry about NLP right now
* Recognise the potential threat to validity, i.e. if someone is criticising government rollout of vaccine but not anti-vaccine
* See the overlap between communities, then fease out individual properties

### Bigger picture of topic
* Why are we doing this, the motivation/importance?
* What are others doing?
* Social media has influence on how information spreads, in this case can be difference between life and death
* Easy to use motivation for this topic
* Tell reviewer you have looked at research

### For the next meeting
* Run the code to collect the data to try identify the misinformation campaigns